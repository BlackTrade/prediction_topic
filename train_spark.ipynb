{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.10\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXECUTORS = 32\n",
    "EXECUTOR_CORES = 4\n",
    "SPARK_NAME = 'chat_bot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.10 (default, Mar 25 2020 23:51:54)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 30 --master yarn --name claims_1 pyspark-shell'\n",
    "\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] =  \"\"\"--master yarn \\\n",
    "                                        --deploy-mode client \\\n",
    "                                        --num-executors {NUM_EXECUTORS} \\\n",
    "                                        --executor-cores {EXECUTOR_CORES} \\\n",
    "                                        --executor-memory 16G \\\n",
    "                                        --driver-memory 4G \\\n",
    "                                        --verbose \\\n",
    "                                        --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./env/bin/python3.6 \\\n",
    "                                        --name {SPARK_NAME} pyspark-shell\n",
    "                                        \"\"\".format(NUM_EXECUTORS = NUM_EXECUTORS, \n",
    "                                                   EXECUTOR_CORES = EXECUTOR_CORES, \n",
    "                                                   SPARK_NAME = SPARK_NAME)\n",
    "\n",
    "PYTHON = 'python3.6'\n",
    "\n",
    "#os.environ[\"PYSPARK_DRIVER_PYTHON\"]='jupyter'\n",
    "#os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]='notebook'\n",
    "os.environ[\"PYSPARK_PYTHON\"] = PYTHON\n",
    "os.environ[\"SPARK_YARN_USER_ENV\"] = PYTHON\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]='/var/local/spark-2.3.2'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://msk-wrk-devgpu01.megafon.ru:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>chat_bot</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe7680b6eb8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addPyFile('/home/pkrylov/packages/numpy-master.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# y_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|LABEL|  INQR_ID|\n",
      "+-----+---------+\n",
      "|    6|252415512|\n",
      "|    0|252975775|\n",
      "|    6|253119730|\n",
      "|    0|253510904|\n",
      "|    5|253540677|\n",
      "+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 3.56 ms, sys: 1.32 ms, total: 4.87 ms\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Y = spark.read.parquet(\"y_concat_inqr_id.parquet.snappy\")#.select(\"label\")\n",
    "Y.show(5, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "CPU times: user 0 ns, sys: 1.71 ms, total: 1.71 ms\n",
      "Wall time: 42.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Y = Y.repartition(250)\n",
    "print(Y.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LABEL: long (nullable = true)\n",
      " |-- INQR_ID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x_coo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency = '1hour'\n",
    "deep = 45\n",
    "\n",
    "# x_all_45_1hour.parquet.snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Start training model with deep 45 latency 1hour\n",
      "+--------+----+------------------+\n",
      "|     row| col|              data|\n",
      "+--------+----+------------------+\n",
      "| 9640570|1985|15.454299926757812|\n",
      "|23706962|1985|22.649999618530273|\n",
      "|24905374|2047| 2.562700033187866|\n",
      "|15668708|2058|0.7383000254631042|\n",
      "| 3453445|1985|21.549400329589844|\n",
      "|27820706|1952|14.759099960327148|\n",
      "|18200664|1955| 37.42190170288086|\n",
      "|22300383|1985| 42.88240051269531|\n",
      "|15851837|2066| 38.49190139770508|\n",
      "|28696207|2049|0.4814999997615814|\n",
      "+--------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 14.6 ms, sys: 15.3 ms, total: 29.9 ms\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# x_all_45_1hour.parquet.snappy\n",
    "result_list = []\n",
    "\n",
    "logging.info(f\"Start training model with deep {deep} latency {latency}\")    \n",
    "#_create_train_test(latency)\n",
    "#_get_metrics()\n",
    "\n",
    "t0 =  time.time()\n",
    "\n",
    "file_name = f\"x_all_{deep}_{latency}.parquet.snappy\"\n",
    "x_coo = spark.read.parquet(file_name)\n",
    "x_coo = x_coo.repartition(250)\n",
    "x_coo.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[row: int, col: int, data: double]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_coo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coorRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Create coorMatrix  498.835 s\n"
     ]
    }
   ],
   "source": [
    "coorRDD = x_coo.rdd.map(lambda x: MatrixEntry(x[0], x[1], x[2])) \n",
    "coorMatrix = CoordinateMatrix(coorRDD)\n",
    "\n",
    "t1 =  time.time()\n",
    "logging.info(\"Create coorMatrix  {:.3f} s\".format(t1 - t0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.mllib.linalg.distributed.CoordinateMatrix at 0x7fe7db188a58>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coorMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Create df from coorMatrix  908.545 s\n",
      "CPU times: user 83.9 ms, sys: 64.8 ms, total: 149 ms\n",
      "Wall time: 14min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = coorMatrix.toRowMatrix().rows.map(lambda x: (x, )).toDF()\n",
    "t2 =  time.time()\n",
    "logging.info(\"Create df from coorMatrix  {:.3f} s\".format(t2 - t1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: vector]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  _1|\n",
      "+--------------------+\n",
      "|(3216,[170,172,17...|\n",
      "|(3216,[170,175,18...|\n",
      "|(3216,[170,173,17...|\n",
      "|(3216,[1229,1241,...|\n",
      "|(3216,[170,172,17...|\n",
      "|(3216,[2,5,8,11,1...|\n",
      "|(3216,[172,175,18...|\n",
      "|(3216,[170,172,18...|\n",
      "|(3216,[170,175,18...|\n",
      "|(3216,[183,184,21...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "\n",
    "#root\n",
    "# |-- features: vector (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Create join Y with df\n",
      "INFO:root:Finished preprocessing df  156.425 s\n"
     ]
    }
   ],
   "source": [
    "get_element=udf(lambda v: int(v[-1]), IntegerType())\n",
    "\n",
    "df = df.withColumn('INQR_ID', get_element('_1'))\n",
    "\n",
    "df = df.join(Y, on = [\"INQR_ID\"]).drop(\"INQR_ID\")\n",
    "logging.info(\"Create join Y with df\") \n",
    "\n",
    "as_ml = udf(lambda v: v.asML() if v is not None else None, VectorUDT())\n",
    "\n",
    "df = df.withColumn(\"features\", as_ml(\"_1\"))\\\n",
    "       .drop(\"_1\") \\\n",
    "       .withColumnRenamed(\"LABEL\", \"label\")\n",
    "\n",
    "t3 =  time.time()\n",
    "logging.info(\"Finished preprocessing df  {:.3f} s\".format(t3 - t2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Create test/train\n",
      "INFO:root:Create train model 823.373 s.\n",
      "CPU times: user 201 ms, sys: 177 ms, total: 378 ms\n",
      "Wall time: 12min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "(train, test) = df.randomSplit([0.85,0.15], seed=25)\n",
    "train = train.repartition(250)\n",
    "logging.info(\"Create test/train\")\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='label', \n",
    "                            featuresCol='features',\n",
    "                            maxDepth=5,\n",
    "                            maxBins=32,\n",
    "                            numTrees=25)\n",
    "model = rf.fit(train)\n",
    "\n",
    "t4 =  time.time()\n",
    "logging.info(\"Create train model {:.3f} s.\".format(t4 - t3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Create predictions  285.568 s.\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test) \n",
    "t5 =  time.time()\n",
    "logging.info(\"Create predictions  {:.3f} s.\".format(t5 - t4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set f1 = 0.12027\n",
      "Test set accuracy = 0.249307\n",
      "INFO:root:Calculate metrics 864.056 s. with latency 1hour\n",
      "CPU times: user 134 ms, sys: 126 ms, total: 260 ms\n",
      "Wall time: 14min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "                   labelCol='label', \n",
    "                   predictionCol='prediction', \n",
    "                   metricName=\"f1\")\n",
    "\n",
    "metric_f1 = evaluator.evaluate(predictions)\n",
    "print(\"Test set f1 = %g\" % metric_f1)\n",
    "\n",
    "evaluator.setMetricName(\"accuracy\")\n",
    "metric_accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = %g\" % metric_accuracy)\n",
    "\n",
    "t6 =  time.time()           \n",
    "logging.info(\"Calculate metrics {:.3f} s.\".format(t6 - t5)) \n",
    "        \n",
    "result_list.append([deep, latency, metric_f1, metric_accuracy])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[45, '1hour', 0.12027046178176957, 0.24930721489322002]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# busines metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.81 ms, sys: 639 µs, total: 6.44 ms\n",
      "Wall time: 176 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "def get_score(x):\n",
    "    len_topn = 5\n",
    "    return  [int(i) for x_, i in sorted(zip(x, range(len(x))), reverse = True)][:len_topn]    \n",
    "\n",
    "get_score_udf = udf(lambda x: get_score(x), ArrayType(IntegerType()))\n",
    "get_score_udf2 = udf(lambda x: len({x[0]}.intersection(set(x[1]))) / 5, FloatType())\n",
    "\n",
    "predictions = predictions.select('label', get_score_udf('probability').alias(\"top5_predict\"))\n",
    "\n",
    "predictions = predictions.withColumn('score_model', get_score_udf2(struct('label', 'top5_predict')))\\\n",
    "                         .withColumn('score1_model', (col(\"score_model\") > 0).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------+------------+\n",
      "|label|top5_predict       |score_model|score1_model|\n",
      "+-----+-------------------+-----------+------------+\n",
      "|0    |[21, 7, 41, 19, 5] |0.0        |0           |\n",
      "|0    |[21, 7, 41, 19, 26]|0.0        |0           |\n",
      "|0    |[21, 7, 41, 36, 5] |0.0        |0           |\n",
      "|0    |[36, 7, 21, 41, 26]|0.0        |0           |\n",
      "|0    |[21, 5, 7, 41, 36] |0.0        |0           |\n",
      "|0    |[21, 7, 41, 36, 19]|0.0        |0           |\n",
      "|0    |[36, 7, 41, 21, 35]|0.0        |0           |\n",
      "|0    |[21, 7, 41, 36, 19]|0.0        |0           |\n",
      "|0    |[21, 7, 41, 5, 36] |0.0        |0           |\n",
      "|0    |[36, 41, 21, 7, 26]|0.0        |0           |\n",
      "|0    |[21, 7, 36, 41, 19]|0.0        |0           |\n",
      "|0    |[21, 7, 41, 5, 19] |0.0        |0           |\n",
      "|0    |[21, 7, 41, 5, 19] |0.0        |0           |\n",
      "|0    |[21, 7, 41, 19, 5] |0.0        |0           |\n",
      "|0    |[21, 7, 41, 18, 36]|0.0        |0           |\n",
      "|0    |[21, 7, 41, 5, 19] |0.0        |0           |\n",
      "|0    |[21, 7, 41, 19, 5] |0.0        |0           |\n",
      "|0    |[21, 7, 41, 5, 19] |0.0        |0           |\n",
      "|0    |[21, 7, 41, 19, 5] |0.0        |0           |\n",
      "|0    |[21, 7, 41, 19, 5] |0.0        |0           |\n",
      "+-----+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 33 ms, sys: 17.5 ms, total: 50.4 ms\n",
      "Wall time: 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41 ms, sys: 39.5 ms, total: 80.5 ms\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "length_test = test.count()\n",
    "length_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.7 ms, sys: 44.7 ms, total: 83.5 ms\n",
      "Wall time: 4min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "metrics = predictions.agg({\"score_model\": \"sum\", \"score1_model\": \"sum\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.608565501869997"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_model = metrics[0][0] / length_test\n",
    "score_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12171310218766593"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1_model = metrics[0][1] / length_test\n",
    "score1_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full short code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'wl - with out latency\n",
    "latency_dict = {'wl':  -1.0,\n",
    "                '15min': 0.010416667,\n",
    "                '30min': 0.020833333,\n",
    "                '1hour': 0.041666667,               \n",
    "                '3hour': 0.125,\n",
    "                '6hour': 0.25,\n",
    "                '12hour': 0.5,\n",
    "                '24hour': 1}\n",
    "\n",
    "#deep_list = [14,45] \n",
    "deep_list = [14] \n",
    "# глубина данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.functions import udf, struct\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Start training model with deep 45 latency 1hour\n",
      "INFO:root:Create coorMatrix  151.918 s\n",
      "INFO:root:Create df from coorMatrix  925.979 s\n",
      "INFO:root:Create join Y with df\n",
      "INFO:root:Finished preprocessing df  0.029 s\n",
      "INFO:root:Create test/train\n",
      "INFO:root:Train 151.918 s. with deep 45 latency 1hour\n",
      "INFO:root:Create train model 777.481 s.\n",
      "INFO:root:Create predictions  0.041 s.\n",
      "Test set f1 = 0.120432\n",
      "Test set accuracy = 0.249443\n",
      "INFO:root:Calculate metrics 901.550 s.\n",
      "Test busines accuracy metrics 0.608458911737076 0.12169178416076408\n",
      "INFO:root:Calculate busines metrics 600.084 s.\n"
     ]
    }
   ],
   "source": [
    "# x_all_45_1hour.parquet.snappy\n",
    "latency = '1hour'\n",
    "deep = 45  \n",
    "\n",
    "logging.info(f\"Start training model with deep {deep} latency {latency}\")   \n",
    "t0 =  time.time()\n",
    "\n",
    "file_name = f\"x_all_{deep}_{latency}.parquet.snappy\"\n",
    "x_coo = spark.read.parquet(file_name)\n",
    "x_coo = x_coo.repartition(250)\n",
    "\n",
    "coorRDD = x_coo.rdd.map(lambda x: MatrixEntry(x[0], x[1], x[2])) \n",
    "coorMatrix = CoordinateMatrix(coorRDD)\n",
    "t1 =  time.time()\n",
    "logging.info(\"Create coorMatrix  {:.3f} s\".format(t1 - t0)) \n",
    "\n",
    "df = coorMatrix.toRowMatrix().rows.map(lambda x: (x, )).toDF()\n",
    "t2 =  time.time()\n",
    "logging.info(\"Create df from coorMatrix  {:.3f} s\".format(t2 - t1)) \n",
    "\n",
    "get_element=udf(lambda v: int(v[-1]), IntegerType())\n",
    "df = df.withColumn('INQR_ID', get_element('_1'))\n",
    "\n",
    "df = df.join(Y, on = [\"INQR_ID\"]).drop(\"INQR_ID\")\n",
    "logging.info(\"Create join Y with df\") \n",
    "\n",
    "as_ml = udf(lambda v: v.asML() if v is not None else None, VectorUDT())\n",
    "df = df.withColumn(\"features\", as_ml(\"_1\"))\\\n",
    "       .drop(\"_1\") \\\n",
    "       .withColumnRenamed(\"LABEL\", \"label\")\n",
    "\n",
    "t3 =  time.time()\n",
    "logging.info(\"Finished preprocessing df  {:.3f} s\".format(t3 - t2)) \n",
    "\n",
    "(train, test) = df.randomSplit([0.85,0.15], seed=25)\n",
    "train = train.repartition(250)\n",
    "logging.info(\"Create test/train\")\n",
    "\n",
    "t4 =  time.time()           \n",
    "logging.info(\"Train {:.3f} s. with deep {} latency {}\".format(t1 - t0, deep, latency)) \n",
    "\n",
    "rf = RandomForestClassifier(labelCol='label', \n",
    "                                    featuresCol='features',\n",
    "                                    maxDepth=5,\n",
    "                                    maxBins=32,\n",
    "                                    numTrees=25)\n",
    "model = rf.fit(train)\n",
    "t4 =  time.time()\n",
    "logging.info(\"Create train model {:.3f} s.\".format(t4 - t3))\n",
    "\n",
    "predictions = model.transform(test)  \n",
    "t5 =  time.time()\n",
    "logging.info(\"Create predictions  {:.3f} s.\".format(t5 - t4))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "                   labelCol='label', \n",
    "                   predictionCol='prediction', \n",
    "                   metricName=\"f1\")\n",
    "\n",
    "metric_f1 = evaluator.evaluate(predictions)\n",
    "print(\"Test set f1 = %g\" % metric_f1)\n",
    "\n",
    "evaluator.setMetricName(\"accuracy\")\n",
    "metric_accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = %g\" % metric_accuracy)\n",
    "\n",
    "t6 =  time.time()           \n",
    "logging.info(\"Calculate metrics {:.3f} s.\".format(t6 - t5)) \n",
    "\n",
    "get_score_udf = udf(lambda x: get_score(x), ArrayType(IntegerType()))\n",
    "get_score_udf2 = udf(lambda x: len({x[0]}.intersection(set(x[1]))) / 5, FloatType())\n",
    "\n",
    "predictions = predictions.select('label', get_score_udf('probability').alias(\"top5_predict\"))\n",
    "\n",
    "predictions = predictions.withColumn('score_model', get_score_udf2(struct('label', 'top5_predict')))\\\n",
    "                         .withColumn('score1_model', (col(\"score_model\") > 0).cast(\"int\"))\n",
    "\n",
    "metrics = predictions.agg({\"score_model\": \"sum\", \"score1_model\": \"sum\"}).collect()\n",
    "length_test = test.count()\n",
    "score_model = metrics[0][0] / length_test\n",
    "score1_model = metrics[0][1] / length_test  \n",
    "print(f\"Test busines accuracy metrics {score_model} {score1_model}\")\n",
    "\n",
    "t7 =  time.time()           \n",
    "logging.info(\"Calculate busines metrics {:.3f} s.\".format(t7 - t6)) \n",
    "        \n",
    "result_list.append([deep, latency, metric_f1, metric_accuracy, score_model, score1_model])                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[45, '1hour', 0.12027046178176957, 0.24930721489322002],\n",
       " [45,\n",
       "  '1hour',\n",
       "  0.1204322793511232,\n",
       "  0.2494428381480604,\n",
       "  0.608458911737076,\n",
       "  0.12169178416076408]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda's Python 3",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "299px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "625px",
    "left": "1538px",
    "right": "20px",
    "top": "113px",
    "width": "357px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
